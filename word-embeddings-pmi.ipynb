{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding Experiments (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import linalg\n",
    "from math import log\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: you can reload the W matrix and the vocab from down below in the line that uses pickle if you dont want to recalculate the matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments should be done with the Wikipedia corpus here:\n",
    "                   /project2/cmsc25025/wikipedia/wiki-text.txt\n",
    "The number of unique words in the Wikipedia corpus is too large for our purposes. Before proceeding, you should come up with a smaller vocabulary V that you will use for the remainder of the word embedding experiments. You can filter all of the unique words in the Wikipedia corpus in a number of ways. Here are some examples:<br>\n",
    "\n",
    "• Remove words that appear less than n times (e.g. try n = 500). You may use any existing python packages to compute word counts, for example nltk.FreqDist or collections.Counter.\n",
    "• Remove all words that appear in the stopwords list of nltk package.<br>\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    nltk.download(’stopwords’)\n",
    "    stop_words = set(stopwords.words(’english’))\n",
    "       \n",
    "You should aim to have roughly 15,000 words in your vocabulary. In what follows, you may simply ignore words that do not appear in your final filtered vocabulary V . Whatever code you run try it first on a small section of the wiki data. The final run may take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open('/project2/cmsc25025/wikipedia/wiki-text.txt') as f:\n",
    "    for ws in f:\n",
    "        words = ws.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_n = 500\n",
    "vocab = list(filter(lambda x: x[1] > required_n and x[0] not in sw, word_counter.items()))\n",
    "vocab = set([x[0] for x in vocab])\n",
    "\n",
    "words = list(filter(lambda x: x in vocab, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Using the Wikipedia data with a symmetric context window size of 5, compute the PMI matrix $M ∈ R^{N(V)×N(V)}$ whose (i,j)-th entry is the PMI of $(w_i,w_j)$. To avoid degeneracy, add 1 to the cooccurence statistics $N^p(w_i,w_j)$ so that\n",
    "$$M_{i j}=\\log \\left(\\frac{\\left(N^{p}\\left(w_{i}, w_{j}\\right)+1\\right) \\cdot N\\left(\\mathcal{S}^{p}\\right)}{N^{p}\\left(w_{i}\\right) \\cdot N^{p}\\left(w_{j}\\right)}\\right)$$\n",
    "#### where $S^p$ is the set of all word-context pairs observed in the Wikipedia corpus. Note that $N^p(w_i,w_j)$ is simply the number of times $w_i$ and $w_j$ cooccur within 5 words of each other across the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createContexts(words):\n",
    "    counter_wc = Counter()\n",
    "    counter_w = Counter()\n",
    "    ns = 0\n",
    "    for (i,w) in enumerate(words):\n",
    "        for j in list(range(max(0, i-5),i)) + list(range(i+1,min(i+6,len(words)))):\n",
    "            counter_wc[(w,words[j])] += 1\n",
    "            counter_w[w] += 1\n",
    "            ns += 1\n",
    "    return counter_wc, counter_w, ns\n",
    "                \n",
    "nwc, nw, ns = createContexts(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMij(wi, wj):\n",
    "    return log(((nwc[(wi,wj)] + 1) * ns)/(nw[wi]*nw[wj]))\n",
    "\n",
    "M = np.array([[getMij(wi,wj) for wj in vocab] for wi in vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (b) Now we will factorize the matrix M to obtain word embeddings. Take the k-SVD of M with k=50\n",
    "$$M=U \\Sigma V^{T}=U \\Sigma^{\\frac{1}{2}} \\Sigma^{\\frac{1}{2}} V^{T}$$\n",
    "#### where $U \\in \\mathbb{R}^{|V| \\times 50}, \\Sigma \\in \\mathbb{R}^{50 \\times 50}$, and $V^{T} \\in \\mathbb{R}^{50 \\times|V|}$ You may use the scipy package to compute the k-SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = linalg.svds(csr_matrix(M), k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (c) Let the rows of $W = UΣ^{1/2}$ be the learned PMI word embeddings. You will use these embeddings in several tasks below. We recommend that you serialize these embeddings to disk for later use (e.g., by using the cPickle package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = U.dot(np.diag(sigma**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(W, open( \"W_PMI.p\", \"wb\" ))\n",
    "pickle.dump(list(vocab), open( \"vocab_PMI.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload from here if you don't want to recalculate the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pickle.load( open( \"W_PMI.p\", \"rb\" ) )\n",
    "vocab = pickle.load( open( \"vocab_PMI.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) For each of the following words, find the 5 closest words in the embedding space: \n",
    "    physics, republican, einstein, algebra, fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClosest(W, vocab, w_key, num = 6):\n",
    "    closest = [(vocab[i], np.linalg.norm(w_key - w)) for (i,w) in enumerate(W)]\n",
    "    closest.sort(key = lambda x: x[1])\n",
    "    \n",
    "    closest = [x[0] for x in closest[:num]]\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 closest words to physics are ['mechanics', 'quantum', 'chemistry', 'theoretical', 'mathematics']\n",
      "The 5 closest words to republican are ['senator', 'democrat', 'democrats', 'candidate', 'presidential']\n",
      "The 5 closest words to einstein are ['relativity', 'physicists', 'paradox', 'maxwell', 'experiment']\n",
      "The 5 closest words to algebra are ['algebraic', 'finite', 'theorem', 'topology', 'calculus']\n",
      "The 5 closest words to fish are ['fruit', 'eggs', 'eat', 'seeds', 'meat']\n"
     ]
    }
   ],
   "source": [
    "key_words = [\"physics\", 'republican', 'einstein', 'algebra', 'fish']\n",
    "close_words = []\n",
    "for key in key_words:\n",
    "    w_key = W[vocab.index(key)]\n",
    "    close_words.append(getClosest(W, vocab, w_key)[1:])\n",
    "for i,key in enumerate(key_words):\n",
    "    print(\"The 5 closest words to\", key, \"are\", close_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all seem like similar words and thus the model performed extremely well in this situation. I was amazed by how amazing this performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) A surprising consequence of some word embedding methods is the resulting linear substructure. This structure can be used to solve analogies like\n",
    "    france :  paris ::  england :  ?\n",
    "#### by computing the nearest embedding vector to v where v is <br>\n",
    "    v = v_{paris} − v_{france} + v_{england}\n",
    "#### Define 3 analogies X:Y=Z:W. and report the top 5 words you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republican : democrat :: conservative : ['conservative', 'liberal', 'opposition', 'leadership', 'leaders']\n",
      "wet : dry :: hot : ['cool', 'smoke', 'flame', 'burn', 'dust']\n",
      "taste : sweet :: smell : ['sensation', 'unpleasant', 'treating', 'smell', 'defects']\n"
     ]
    }
   ],
   "source": [
    "def getAnalogyV(x, y, z):\n",
    "    wx = W[vocab.index(x)]\n",
    "    wy = W[vocab.index(y)]\n",
    "    wz = W[vocab.index(z)]\n",
    "    return wx - wy + wz\n",
    "\n",
    "analogy_words = [[\"republican\", \"democrat\", \"conservative\"], [\"wet\", \"dry\", \"hot\"],[\"taste\", \"sweet\", \"smell\"]]\n",
    "closest_analogy = []\n",
    "for i in range(len(analogy_words)):\n",
    "    v = getAnalogyV(analogy_words[i][0], analogy_words[i][1], analogy_words[i][2])\n",
    "    closest_analogy.append(getClosest(W, vocab, v, num = 5))\n",
    "for i,analogy in enumerate(analogy_words):\n",
    "    print(analogy[0],\":\",analogy[1],\"::\",analogy[2],\":\",closest_analogy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analogies did not perform as well as the similar words, but the words returned are still defintely relevant to each other, especially the first few. It is hard to have this model return a strong analogy given the fact that the model only looks at similar relevant words and not definitions or any other sort of relevant metadata. Many words have multiple uses and meanings, which makes it difficult to judge the word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_GPU_cuda_9.0]",
   "language": "python",
   "name": "conda-env-DL_GPU_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
